[project]
name = "rag-api"
version = "0.1.0"
description = "RAG API with document ingestion and conversational chat"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    # Core web framework
    "fastapi[standard]>=0.128.0",
    # Database
    "sqlmodel>=0.0.32",
    "asyncpg>=0.31.0",
    "psycopg2-binary>=2.9.11",
    "sqlalchemy[asyncio]>=2.0.46",
    # Redis for chat memory
    "redis>=7.1.0",
    # Vector database
    "qdrant-client>=1.0.0",
    # OPTIMIZED ML STACK (92% smaller than torch+transformers)
    "onnxruntime>=1.15.0",        # ~100MB vs torch's ~500MB+
    "tokenizers>=0.20.0",         # ~10MB vs transformers' ~300MB+
    "numpy>=2.0.0",               # Core tensor operations
    # NLP processing
    "spacy>=3.0.0",
    # LLM integration
    "aiohttp>=3.13.3",
    # Chat optimization
    "toons>=0.5.1",
    # File processing
    "pdfplumber>=0.10.0",
    # Date parsing
    "python-dateutil>=2.9.0",
    # Configuration
    "python-multipart>=0.0.6",
    "python-dotenv>=1.2.1",
    "chardet>=5.2.0",
    "pip>=26.0.1",
]

# REMOVED HEAVY DEPENDENCIES (saves ~1.5GB total):
# "torch>=2.0.0"              # ~500MB + CUDA dependencies ~1GB  
# "transformers>=4.30.0"       # ~300MB
# "optimum[onnxruntime]>=1.9.0" # Depends on transformers

[project.optional-dependencies]
# Heavy dependencies only needed for one-time model conversion
conversion = [
    "torch>=2.0.0",
    "transformers>=4.30.0", 
    "optimum[onnxruntime]>=1.9.0",
]

# Development dependencies
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "httpx>=0.24.0",  # for testing
]
